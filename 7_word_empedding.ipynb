{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings in NLP are a type of word representation that allows words to be represented as vectors of real numbers. These vectors are learned from large corpora of text, where semantically similar words are represented by similar vectors. Word embeddings capture semantic relationships between words, such as synonyms and antonyms, by mapping words to dense, continuous vector spaces.\n",
    "\n",
    "Some of the most popular word embedding models include:\n",
    "\n",
    "- Word2Vec (Skip-gram and CBOW models)\n",
    "- GloVe (Global Vectors for Word Representation)\n",
    "- FastText\n",
    "- BERT and other contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (Skip-gram and CBOW models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Skip-gram model is designed to predict the context words (surrounding words) given a target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Objective: For a given target word, predict the surrounding words (context words) within a specified window size.\n",
    "- How it works: Given a word (target), the model predicts the words that are most likely to occur near it in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CBOW model works in the opposite direction to the Skip-gram model. It tries to predict the target word given a set of context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), Bag of Words (BoW) is a simple and commonly used method for text representation. It is a technique that transforms text data into numerical features, allowing it to be used in machine learning models. The key idea behind BoW is to represent a document as a collection (bag) of words without considering the order or structure of those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization: Text is broken down into individual words (tokens). Punctuation marks, case sensitivity, and stopwords (like \"the\", \"and\", \"is\") are often removed or handled before this step.\n",
    "\n",
    "- Vocabulary Creation: A vocabulary is created by extracting all unique words from the entire dataset. Each unique word in the corpus is assigned a specific index in a vector.\n",
    "\n",
    "- Document Representation: Each document is represented as a vector of the same length as the vocabulary. The values in the vector typically represent the frequency of each word in the document or sometimes the binary presence (1 if the word is present, 0 if it is not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "Consider a small corpus with the following two documents:\n",
    "\n",
    "1. \"I love NLP\"\n",
    "2. \"I love machine learning\"\n",
    "#### Step 1: Tokenization\n",
    "- Document 1: [\"I\", \"love\", \"NLP\"]\n",
    "- Document 2: [\"I\", \"love\", \"machine\", \"learning\"]\n",
    "#### Step 2: Create Vocabulary\n",
    "Vocabulary: [\"I\", \"love\", \"NLP\", \"machine\", \"learning\"]\n",
    "\n",
    "#### Step 3: Represent Documents as Vectors\n",
    "- Document 1: [1, 1, 1, 0, 0] (1 for \"I\", 1 for \"love\", 1 for \"NLP\", and 0 for \"machine\", \"learning\")\n",
    "- Document 2: [1, 1, 0, 1, 1] (1 for \"I\", 1 for \"love\", 0 for \"NLP\", and 1 for \"machine\", \"learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Works well in many cases, especially with small to medium-sized datasets.\n",
    "Can be used in models like Naive Bayes, Logistic Regression, and Support Vector Machines.\n",
    "#### Cons:\n",
    "\n",
    "Ignores word order and grammar.\n",
    "Leads to sparse vectors, which can be computationally expensive when the vocabulary is large.\n",
    "Does not capture semantic meaning or context of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhancements:\n",
    "To improve the basic BoW, various enhancements can be made, such as:\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): Weighs words based on their frequency within a document and across the corpus, reducing the impact of common words.\n",
    "N-grams: Instead of using individual words, combinations of words (like bigrams or trigrams) can be used to capture more context.\n",
    "BoW is often the first step in transforming text data into a form that machine learning models can work with."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
